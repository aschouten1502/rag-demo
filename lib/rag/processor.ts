/**
 * ========================================
 * SUPABASE RAG - Document Processor
 * ========================================
 *
 * Verwerkt PDF uploads:
 * 1. Extraheert tekst per pagina
 * 2. Chunked de content
 * 3. Genereert embeddings
 * 4. Slaat op in Supabase
 *
 * Ondersteunde formaten:
 * - PDF (.pdf)
 */

import { createClient, SupabaseClient } from '@supabase/supabase-js';
import { smartChunkDocument, chunkDocument } from './chunking';
import { generateEmbeddingsBatch } from './embeddings';
import { generateMetadataBatch, isMetadataGenerationEnabled } from './metadata-generator';
import { Document, ProcessingResult, EnhancedChunkMetadata, SmartChunkingOptions, StructuredChunk } from './types';
import { getDocumentsBucket, ensureBucketExists } from '../admin/storage-service';
import {
  startDocumentProcessing,
  updateProcessingStatus,
  completeDocumentProcessing,
  failDocumentProcessing
} from '../admin/document-processing-logger';

// ========================================
// SMART CHUNKING CONFIG
// ========================================

/**
 * Check if smart chunking is enabled
 * Enable via environment variable or default to true
 */
function isSmartChunkingEnabled(): boolean {
  const envValue = process.env.ENABLE_SMART_CHUNKING;
  // Default to true (enabled)
  return envValue !== 'false';
}

/**
 * Get smart chunking options from environment
 */
function getSmartChunkingOptions(): Partial<SmartChunkingOptions> {
  return {
    enableStructureDetection: process.env.SMART_CHUNK_STRUCTURE !== 'false',
    enableSemanticChunking: process.env.SMART_CHUNK_SEMANTIC !== 'false',
    enableContextHeaders: process.env.SMART_CHUNK_HEADERS !== 'false',
    enableSmartBoundaries: process.env.SMART_CHUNK_BOUNDARIES !== 'false',
    semanticModel: (process.env.SMART_CHUNK_MODEL as 'gpt-4o-mini' | 'gpt-4o') || 'gpt-4o-mini'
  };
}

// ========================================
// SUPABASE CLIENT
// ========================================

let supabaseClient: SupabaseClient | null = null;

function getSupabaseClient(): SupabaseClient {
  if (!supabaseClient) {
    const url = process.env.NEXT_PUBLIC_SUPABASE_URL;
    const key = process.env.SUPABASE_SERVICE_ROLE_KEY;

    if (!url || !key) {
      throw new Error('Supabase configuration missing');
    }

    supabaseClient = createClient(url, key);
  }
  return supabaseClient;
}

// ========================================
// PDF TEXT EXTRACTION
// ========================================

/**
 * Result type voor PDF extractie met echte page count
 */
interface PDFExtractionResult {
  pages: Array<{ pageNumber: number; text: string }>;
  totalPages: number;  // Echte page count van de PDF
}

/**
 * Extraheert tekst uit een PDF buffer
 * Gebruikt pdf-parse v1.x voor server-side processing
 *
 * BELANGRIJK: Returns zowel ge√´xtraheerde pagina's als de ECHTE page count
 * De pages array kan kleiner zijn (lege pagina's worden gefilterd)
 * maar totalPages is altijd de werkelijke PDF page count
 */
async function extractTextFromPDF(
  buffer: Buffer
): Promise<PDFExtractionResult> {
  // pdf-parse v1.x - directe functie call (geen workers nodig)
  // eslint-disable-next-line @typescript-eslint/no-require-imports
  const pdfParse = require('pdf-parse');

  console.log('üìñ [Processor] Parsing PDF...');

  const pdfData = await pdfParse(buffer);

  // BELANGRIJK: Gebruik de ECHTE page count van de PDF metadata
  const totalPages = pdfData.numpages || 1;

  // v1.x geeft { text, numpages, info } terug
  const fullText = pdfData.text || '';

  // Split op form feed (\f) of meerdere newlines (pagina-einde indicatie)
  const pageTexts = fullText.split(/\f|\n{4,}/);

  const pages: Array<{ pageNumber: number; text: string }> = [];

  pageTexts.forEach((pageText: string, idx: number) => {
    const trimmed = pageText.trim();
    if (trimmed.length > 0) {
      pages.push({
        pageNumber: idx + 1,
        text: trimmed
      });
    }
  });

  console.log(`‚úÖ [Processor] Extracted ${pages.length} text segments from ${totalPages} PDF pages`);

  // Fallback: als geen pagina's gevonden, gebruik volledige tekst
  if (pages.length === 0 && fullText.trim().length > 0) {
    return {
      pages: [{
        pageNumber: 1,
        text: fullText.trim()
      }],
      totalPages
    };
  }

  if (pages.length === 0) {
    throw new Error('No text content could be extracted from PDF');
  }

  return { pages, totalPages };
}

// ========================================
// MAIN PROCESSING FUNCTION
// ========================================

/**
 * Verwerkt een PDF bestand en indexeert het in de RAG database
 *
 * @param tenantId - Tenant identifier
 * @param filename - Naam van het bestand
 * @param fileBuffer - Buffer met de file content
 * @param filePath - Optioneel pad in Supabase Storage
 * @returns Processing resultaat met document ID en stats
 */
export async function processDocument(
  tenantId: string,
  filename: string,
  fileBuffer: Buffer,
  filePath?: string
): Promise<ProcessingResult> {
  const supabase = getSupabaseClient();

  console.log('\nüìÑ [Processor] ========== PROCESSING DOCUMENT ==========');
  console.log('üìÅ [Processor] File:', filename);
  console.log('üè¢ [Processor] Tenant:', tenantId);
  console.log('üì¶ [Processor] Size:', (fileBuffer.length / 1024).toFixed(1), 'KB');

  let documentId: string | undefined;
  let processingLogId: string | null = null;

  // Start document processing log
  const smartChunkingEnabled = isSmartChunkingEnabled();
  const smartOptions = smartChunkingEnabled ? getSmartChunkingOptions() : undefined;

  processingLogId = await startDocumentProcessing({
    tenantId,
    filename,
    fileSizeBytes: fileBuffer.length,
    mimeType: 'application/pdf',
    chunkingMethod: smartChunkingEnabled ? 'smart' : 'legacy',
    chunkingOptions: smartOptions ? {
      structureDetection: smartOptions.enableStructureDetection,
      semanticChunking: smartOptions.enableSemanticChunking,
      contextHeaders: smartOptions.enableContextHeaders,
      smartBoundaries: smartOptions.enableSmartBoundaries
    } : undefined
  });

  try {
    // 1. Maak document record aan
    const { data: doc, error: docError } = await supabase
      .from('documents')
      .insert({
        tenant_id: tenantId,
        filename,
        file_path: filePath,
        file_size: fileBuffer.length,
        processing_status: 'processing'
      })
      .select()
      .single();

    if (docError) throw docError;
    documentId = doc.id;

    console.log('‚úÖ [Processor] Document record created:', documentId);

    // Update log with document ID
    if (processingLogId) {
      await updateProcessingStatus(processingLogId, 'parsing');
    }

    // 1b. Upload PDF to Supabase Storage (tenant-specific bucket)
    const bucketName = getDocumentsBucket(tenantId);
    const storagePath = `documents/${documentId}/${filename}`;
    console.log(`üì§ [Processor] Uploading PDF to Storage: bucket=${bucketName}, path=${storagePath}`);

    // Ensure bucket exists before uploading
    await ensureBucketExists(bucketName, true);

    const { error: storageError } = await supabase.storage
      .from(bucketName)
      .upload(storagePath, fileBuffer, {
        contentType: 'application/pdf',
        upsert: true
      });

    if (storageError) {
      console.warn('‚ö†Ô∏è [Processor] Storage upload failed:', storageError.message);
      // Continue processing - storage is optional enhancement
    } else {
      console.log('‚úÖ [Processor] PDF uploaded to Storage');
      // Update document with file_path
      await supabase
        .from('documents')
        .update({ file_path: storagePath })
        .eq('id', documentId);
    }

    // 2. Extract text from PDF
    const { pages, totalPages } = await extractTextFromPDF(fileBuffer);

    if (pages.length === 0) {
      throw new Error('No text content could be extracted from PDF');
    }

    // 3. Chunk de content (Smart Chunking of Legacy)
    console.log('\n‚úÇÔ∏è  [Processor] Chunking content...');

    // Update log status to chunking - use REAL page count
    if (processingLogId) {
      await updateProcessingStatus(processingLogId, 'chunking', {
        totalPages: totalPages  // Echte page count, niet gefilterde
      });
    }

    let chunkContents: string[];
    let chunkMetadata: Array<{
      pageNumber?: number;
      chunkIndex: number;
      startChar: number;
      endChar: number;
      wordCount: number;
      structureType?: string;
      structurePath?: string[];
      contextHeader?: string;
    }>;
    let chunkingCost = 0;
    let chunkingTokens = 0;

    if (isSmartChunkingEnabled()) {
      // Smart Chunking met alle 4 opties
      console.log('üß† [Processor] Using SMART CHUNKING (all 4 options)');
      const smartOptions = getSmartChunkingOptions();

      const smartResult = await smartChunkDocument(pages, filename, smartOptions);

      if (smartResult.chunks.length === 0) {
        throw new Error('No chunks could be created from PDF content');
      }

      // Extract content met context header prepended
      chunkContents = smartResult.chunks.map(chunk => {
        if (chunk.contextHeader) {
          return `${chunk.contextHeader}\n\n${chunk.content}`;
        }
        return chunk.content;
      });

      chunkMetadata = smartResult.chunks.map(chunk => ({
        pageNumber: chunk.pageNumber,
        chunkIndex: chunk.chunkIndex,
        startChar: chunk.metadata.startChar,
        endChar: chunk.metadata.endChar,
        wordCount: chunk.metadata.wordCount,
        structureType: chunk.metadata.structureType,
        structurePath: chunk.metadata.structurePath,
        contextHeader: chunk.contextHeader
      }));

      chunkingCost = smartResult.cost;
      chunkingTokens = smartResult.tokensUsed;

      console.log(`‚úÖ [Processor] Smart chunking: ${smartResult.chunks.length} chunks, ${smartResult.structuresDetected} structures`);
      console.log(`üíµ [Processor] Semantic chunking cost: $${chunkingCost.toFixed(4)}`);

    } else {
      // Legacy chunking
      console.log('üì¶ [Processor] Using LEGACY chunking');
      const legacyChunks = chunkDocument(pages);

      if (legacyChunks.length === 0) {
        throw new Error('No chunks could be created from PDF content');
      }

      chunkContents = legacyChunks.map(c => c.content);
      chunkMetadata = legacyChunks.map(chunk => ({
        pageNumber: chunk.pageNumber,
        chunkIndex: chunk.chunkIndex,
        startChar: chunk.metadata.startChar,
        endChar: chunk.metadata.endChar,
        wordCount: chunk.metadata.wordCount
      }));

      console.log(`‚úÖ [Processor] Legacy chunking: ${legacyChunks.length} chunks`);
    }

    // 4. Genereer AI metadata (optioneel - additioneel aan smart chunking)
    let chunkMetadataMap = new Map<number, EnhancedChunkMetadata>();
    let metadataCost = 0;

    if (isMetadataGenerationEnabled()) {
      console.log('\nüß† [Processor] Generating AI metadata...');

      const chunksWithIndex = chunkContents.map((content, idx) => ({
        content,
        index: idx
      }));

      const metadataResult = await generateMetadataBatch(
        chunksWithIndex,
        filename  // Document context
      );

      chunkMetadataMap = metadataResult.results;
      metadataCost = metadataResult.totalCost;

      console.log(`‚úÖ [Processor] Generated metadata for ${chunkMetadataMap.size} chunks`);
      console.log(`üíµ [Processor] Metadata cost: $${metadataCost.toFixed(4)}`);
    } else {
      console.log('\n‚è≠Ô∏è  [Processor] Skipping AI metadata (disabled)');
    }

    // 5. Genereer embeddings
    console.log('\nüî¢ [Processor] Generating embeddings...');

    // Update log status to embedding
    if (processingLogId) {
      await updateProcessingStatus(processingLogId, 'embedding', {
        chunksCreated: chunkContents.length,
        metadataGenerated: isMetadataGenerationEnabled()
      });
    }

    const { embeddings, totalTokens, totalCost } = await generateEmbeddingsBatch(
      chunkContents
    );

    // 6. Sla chunks op in database
    console.log('\nüíæ [Processor] Saving to database...');

    const chunkRecords = chunkContents.map((content, idx) => {
      const enhancedMetadata = chunkMetadataMap.get(idx) || {};
      const meta = chunkMetadata[idx];

      return {
        tenant_id: tenantId,
        document_id: documentId,
        content: content,
        embedding: `[${embeddings[idx].join(',')}]`,
        page_number: meta.pageNumber,
        chunk_index: meta.chunkIndex,
        metadata: {
          startChar: meta.startChar,
          endChar: meta.endChar,
          wordCount: meta.wordCount,
          structureType: meta.structureType,
          structurePath: meta.structurePath,
          contextHeader: meta.contextHeader,
          ...enhancedMetadata     // summary, keywords, topics, alternativeTerms
        }
      };
    });

    // Insert in batches van 50 (Supabase heeft limiet)
    const BATCH_SIZE = 50;
    for (let i = 0; i < chunkRecords.length; i += BATCH_SIZE) {
      const batch = chunkRecords.slice(i, i + BATCH_SIZE);
      const batchNum = Math.floor(i / BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(chunkRecords.length / BATCH_SIZE);

      const { error: insertError } = await supabase
        .from('document_chunks')
        .insert(batch);

      if (insertError) throw insertError;

      console.log(`   Saved batch ${batchNum}/${totalBatches}`);
    }

    // 7. Update document status - use REAL page count
    await supabase
      .from('documents')
      .update({
        total_pages: totalPages,  // Echte page count van PDF
        total_chunks: chunkContents.length,
        processing_status: 'completed'
      })
      .eq('id', documentId);

    const finalCost = totalCost + metadataCost + chunkingCost;

    console.log('\n‚úÖ [Processor] ========== PROCESSING COMPLETE ==========');
    console.log('üìÑ [Processor] Document ID:', documentId);
    console.log('üìä [Processor] Pages:', totalPages);
    console.log('üìä [Processor] Chunks created:', chunkContents.length);
    console.log('üî¢ [Processor] Tokens used:', totalTokens + chunkingTokens);
    console.log('üíµ [Processor] Smart chunking cost: $' + chunkingCost.toFixed(4));
    console.log('üíµ [Processor] Embedding cost: $' + totalCost.toFixed(4));
    console.log('üíµ [Processor] Metadata cost: $' + metadataCost.toFixed(4));
    console.log('üíµ [Processor] Total cost: $' + finalCost.toFixed(4));

    // Complete document processing log with properly separated costs
    if (processingLogId) {
      const chunkSizes = chunkContents.map(c => c.length);
      await completeDocumentProcessing(processingLogId, {
        totalPages: totalPages,  // Echte page count van PDF
        chunksCreated: chunkContents.length,
        structuresDetected: smartChunkingEnabled ? (chunkMetadata.filter(m => m.structureType).length) : 0,
        avgChunkSize: Math.round(chunkSizes.reduce((sum, size) => sum + size, 0) / chunkSizes.length),
        minChunkSize: Math.min(...chunkSizes),
        maxChunkSize: Math.max(...chunkSizes),
        embeddingTokens: totalTokens,
        embeddingCost: totalCost,
        chunkingTokens: chunkingTokens,  // Semantic chunking tokens (separate from metadata)
        chunkingCost: chunkingCost,      // Semantic chunking cost (separate from metadata)
        metadataTokens: 0,               // TODO: Track actual metadata tokens from generateMetadataBatch
        metadataCost: metadataCost,      // Only metadata generation cost (no chunking)
        metadataGenerated: isMetadataGenerationEnabled()
      });
    }

    return {
      success: true,
      documentId,
      chunksCreated: chunkContents.length,
      totalTokens: totalTokens + chunkingTokens,
      totalCost: finalCost,
      metadataCost: metadataCost,   // Only metadata cost (chunking is separate)
      chunkingCost: chunkingCost    // Semantic chunking cost (separate)
    };

  } catch (error) {
    console.error('‚ùå [Processor] Processing failed:', error);

    const errorMessage = error instanceof Error ? error.message : 'Unknown error';

    // Log failure to document processing logs
    if (processingLogId) {
      await failDocumentProcessing(processingLogId, errorMessage, 'processing');
    }

    // Update document status to failed
    if (documentId) {
      await supabase
        .from('documents')
        .update({
          processing_status: 'failed',
          processing_error: errorMessage
        })
        .eq('id', documentId);
    }

    return {
      success: false,
      documentId,
      chunksCreated: 0,
      totalTokens: 0,
      totalCost: 0,
      error: errorMessage
    };
  }
}

// ========================================
// DOCUMENT MANAGEMENT
// ========================================

/**
 * Verwijdert een document en alle chunks
 *
 * @param tenantId - Tenant identifier
 * @param documentId - Document ID om te verwijderen
 * @returns True als succesvol
 */
export async function deleteDocument(
  tenantId: string,
  documentId: string
): Promise<boolean> {
  const supabase = getSupabaseClient();

  console.log('üóëÔ∏è  [Processor] Deleting document:', documentId);

  // CASCADE delete verwijdert automatisch de chunks
  const { error } = await supabase
    .from('documents')
    .delete()
    .eq('id', documentId)
    .eq('tenant_id', tenantId);

  if (error) {
    console.error('‚ùå [Processor] Delete failed:', error);
    return false;
  }

  console.log('‚úÖ [Processor] Document deleted:', documentId);
  return true;
}

/**
 * Lijst alle documenten voor een tenant
 *
 * @param tenantId - Tenant identifier
 * @returns Array van Document objecten
 */
export async function listDocuments(tenantId: string): Promise<Document[]> {
  const supabase = getSupabaseClient();

  const { data, error } = await supabase
    .from('documents')
    .select('*')
    .eq('tenant_id', tenantId)
    .order('created_at', { ascending: false });

  if (error) {
    console.error('‚ùå [Processor] List failed:', error);
    return [];
  }

  return data || [];
}

/**
 * Haalt een enkel document op
 *
 * @param tenantId - Tenant identifier
 * @param documentId - Document ID
 * @returns Document of null
 */
export async function getDocument(
  tenantId: string,
  documentId: string
): Promise<Document | null> {
  const supabase = getSupabaseClient();

  const { data, error } = await supabase
    .from('documents')
    .select('*')
    .eq('id', documentId)
    .eq('tenant_id', tenantId)
    .single();

  if (error) {
    console.error('‚ùå [Processor] Get document failed:', error);
    return null;
  }

  return data;
}

/**
 * Herverwerkt een document (verwijder chunks en maak opnieuw)
 *
 * @param tenantId - Tenant identifier
 * @param documentId - Document ID
 * @param fileBuffer - Nieuwe file buffer
 * @returns Processing resultaat
 */
export async function reprocessDocument(
  tenantId: string,
  documentId: string,
  fileBuffer: Buffer
): Promise<ProcessingResult> {
  const supabase = getSupabaseClient();

  // Haal document metadata op
  const document = await getDocument(tenantId, documentId);
  if (!document) {
    return {
      success: false,
      chunksCreated: 0,
      totalTokens: 0,
      totalCost: 0,
      error: 'Document not found'
    };
  }

  console.log('üîÑ [Processor] Reprocessing document:', documentId);

  // Verwijder bestaande chunks
  const { error: deleteError } = await supabase
    .from('document_chunks')
    .delete()
    .eq('document_id', documentId);

  if (deleteError) {
    console.error('‚ùå [Processor] Failed to delete existing chunks:', deleteError);
  }

  // Update status naar processing
  await supabase
    .from('documents')
    .update({
      processing_status: 'processing',
      processing_error: null
    })
    .eq('id', documentId);

  // Verwerk opnieuw (hergebruik bestaande logic met smart chunking)
  try {
    const { pages, totalPages } = await extractTextFromPDF(fileBuffer);

    // Smart Chunking of Legacy
    let chunkContents: string[];
    let chunkMetadata: Array<{
      pageNumber?: number;
      chunkIndex: number;
      startChar: number;
      endChar: number;
      wordCount: number;
      structureType?: string;
      structurePath?: string[];
      contextHeader?: string;
    }>;
    let chunkingCost = 0;

    if (isSmartChunkingEnabled()) {
      console.log('üß† [Processor] Using SMART CHUNKING for reprocess...');
      const smartOptions = getSmartChunkingOptions();
      const smartResult = await smartChunkDocument(pages, document.filename, smartOptions);

      chunkContents = smartResult.chunks.map(chunk =>
        chunk.contextHeader ? `${chunk.contextHeader}\n\n${chunk.content}` : chunk.content
      );

      chunkMetadata = smartResult.chunks.map(chunk => ({
        pageNumber: chunk.pageNumber,
        chunkIndex: chunk.chunkIndex,
        startChar: chunk.metadata.startChar,
        endChar: chunk.metadata.endChar,
        wordCount: chunk.metadata.wordCount,
        structureType: chunk.metadata.structureType,
        structurePath: chunk.metadata.structurePath,
        contextHeader: chunk.contextHeader
      }));

      chunkingCost = smartResult.cost;
    } else {
      const legacyChunks = chunkDocument(pages);
      chunkContents = legacyChunks.map(c => c.content);
      chunkMetadata = legacyChunks.map(chunk => ({
        pageNumber: chunk.pageNumber,
        chunkIndex: chunk.chunkIndex,
        startChar: chunk.metadata.startChar,
        endChar: chunk.metadata.endChar,
        wordCount: chunk.metadata.wordCount
      }));
    }

    // Genereer AI metadata (optioneel)
    let chunkMetadataMap = new Map<number, EnhancedChunkMetadata>();
    let metadataCost = 0;

    if (isMetadataGenerationEnabled()) {
      console.log('üß† [Processor] Generating AI metadata for reprocess...');

      const chunksWithIndex = chunkContents.map((content, idx) => ({
        content,
        index: idx
      }));

      const metadataResult = await generateMetadataBatch(
        chunksWithIndex,
        document.filename
      );

      chunkMetadataMap = metadataResult.results;
      metadataCost = metadataResult.totalCost;
    }

    const { embeddings, totalTokens, totalCost } = await generateEmbeddingsBatch(
      chunkContents
    );

    const chunkRecords = chunkContents.map((content, idx) => {
      const enhancedMetadata = chunkMetadataMap.get(idx) || {};
      const meta = chunkMetadata[idx];

      return {
        tenant_id: tenantId,
        document_id: documentId,
        content: content,
        embedding: `[${embeddings[idx].join(',')}]`,
        page_number: meta.pageNumber,
        chunk_index: meta.chunkIndex,
        metadata: {
          startChar: meta.startChar,
          endChar: meta.endChar,
          wordCount: meta.wordCount,
          structureType: meta.structureType,
          structurePath: meta.structurePath,
          contextHeader: meta.contextHeader,
          ...enhancedMetadata
        }
      };
    });

    const BATCH_SIZE = 50;
    for (let i = 0; i < chunkRecords.length; i += BATCH_SIZE) {
      const batch = chunkRecords.slice(i, i + BATCH_SIZE);
      await supabase.from('document_chunks').insert(batch);
    }

    await supabase
      .from('documents')
      .update({
        total_pages: totalPages,  // Echte page count van PDF
        total_chunks: chunkContents.length,
        processing_status: 'completed'
      })
      .eq('id', documentId);

    const finalCost = totalCost + metadataCost + chunkingCost;
    console.log('‚úÖ [Processor] Reprocessing complete');

    return {
      success: true,
      documentId,
      chunksCreated: chunkContents.length,
      totalTokens,
      totalCost: finalCost,
      metadataCost: metadataCost + chunkingCost
    };

  } catch (error) {
    await supabase
      .from('documents')
      .update({
        processing_status: 'failed',
        processing_error: error instanceof Error ? error.message : 'Unknown error'
      })
      .eq('id', documentId);

    return {
      success: false,
      documentId,
      chunksCreated: 0,
      totalTokens: 0,
      totalCost: 0,
      error: error instanceof Error ? error.message : 'Unknown error'
    };
  }
}
